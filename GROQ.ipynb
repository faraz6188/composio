{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadbf992-c983-42c3-aa6e-2d60e93e7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d2e1be-ebe0-45eb-a974-757cb9f4e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82a1a27-b5cd-48fc-ba6d-2d78d75eb9ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgroq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[0;32m----> 5\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgsk_03AIkVbRAADce8tHjYydWGdyb3FYWzNO4J3zRWfxKP4YWJ8XVJx6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     11\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-8b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/groq/_client.py:89\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     87\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"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\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73ba96-24bf-405b-a567-69d60c6b6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\",  # Direct API key\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the GROQ  to a 10 year old\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a3b83-5faa-45a9-9212-a986414329e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the GROQ  to a 10 year old\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80e4db-0f93-4b91-b35a-e53fe644b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client with the API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Model name (ensure this model exists and is compatible with the Groq API)\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "# Initialize the history list to store the conversation\n",
    "history = []\n",
    "\n",
    "# Initialize the chat session with the model\n",
    "chat = client.start_chat(model=model_name, history=history)\n",
    "\n",
    "# Main loop to interact with the chat model\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your message (or type 'exit' to stop): \")\n",
    "\n",
    "    # Break loop if the user wants to exit\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Append the user input to the history\n",
    "    history.append({\"role\": \"user\", \"parts\": user_input})\n",
    "    \n",
    "    # Send the user's message to the chat and get the response\n",
    "    response = chat.send_message(user_input)\n",
    "    \n",
    "    # Append the model's response to the history\n",
    "    history.append({\"role\": \"model\", \"parts\": response.text})\n",
    "\n",
    "    # Print the model's response\n",
    "    print(f\"Model: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b4efb-d328-4ef8-8e11-574a3a3167b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client with the API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Model name (ensure this model exists and is compatible with the Groq API)\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "# Initialize the history list to store the conversation\n",
    "history = []\n",
    "\n",
    "# Main loop to interact with the chat model\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your message (or type 'exit' to stop): \")\n",
    "\n",
    "    # Break loop if the user wants to exit\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    # Append the user input to the history\n",
    "    history.append({\"role\": \"user\", \"parts\": user_input})\n",
    "    \n",
    "    try:\n",
    "        # Assuming Groq has a method like 'predict' or 'run_model'\n",
    "        # Replace 'predict' with the actual method name used in Groq's API\n",
    "        response = client.predict(model_name=model_name, input_text=user_input)\n",
    "        \n",
    "        # Append the model's response to the history\n",
    "        history.append({\"role\": \"model\", \"parts\": response['output']})\n",
    "\n",
    "        # Print the model's response\n",
    "        print(f\"Model: {response['output']}\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "from groq import Groq  # Import the Groq library\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Define the model you want to use\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "# Start an empty history list to store conversations\n",
    "history = []\n",
    "\n",
    "# Main loop to interact with the model\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your message (or type 'exit' to stop): \")\n",
    "\n",
    "    # Exit the loop if the user types 'exit'\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Append user input to the history\n",
    "    history.append({\"role\": \"user\", \"parts\": user_input})\n",
    "\n",
    "    try:\n",
    "        # Assuming Groq has a method like 'run_model' or 'predict'\n",
    "        # Replace this with the actual method name from Groq API docs\n",
    "        response = client.run_model(\n",
    "            model_name=model_name,    # Model to use\n",
    "            input_data=user_input     # The input from the user\n",
    "        )\n",
    "\n",
    "        # Append the model's response to the history\n",
    "        history.append({\"role\": \"model\", \"parts\": response['output']})\n",
    "\n",
    "        # Print the model's response\n",
    "        print(f\"Model: {response['output']}\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ef058-1183-4fa4-a3e4-d3e145da707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq  # Import the Groq library\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Define the model you want to use\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "# Initialize the history list to store the conversation\n",
    "history = []\n",
    "\n",
    "# Main loop to interact with the chat model\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter your message (or type 'exit' to stop): \")\n",
    "\n",
    "    # Break loop if the user wants to exit\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Append the user input to the history\n",
    "    history.append({\"role\": \"user\", \"parts\": user_input})\n",
    "    \n",
    "    try:\n",
    "        # Assuming Groq has a method like 'run_model' or 'execute'\n",
    "        # Replace 'run_model' with the actual method name from Groq API documentation\n",
    "        response = client.run_model(model_name=model_name, input_data=user_input)\n",
    "        \n",
    "        # Append the model's response to the history\n",
    "        history.append({\"role\": \"model\", \"parts\": response['output']})\n",
    "\n",
    "        # Print the model's response\n",
    "        print(f\"Model: {response['output']}\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09797364-4745-4606-8ef8-25d034ab6618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask Q or exit:  Explain Jira\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Jira!\n",
      "\n",
      "Jira is a popular project management and issue tracking tool developed by Atlassian. It's widely used by software development teams, DevOps teams, and other businesses to plan, track, and manage their work. Here's a concise overview:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Issue tracking**: Create and track issues, bugs, tasks, and projects. Assign them to team members, set deadlines, and monitor progress.\n",
      "2. **Project management**: Organize your work into projects,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask Q or exit:  exit\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq  # Import the Groq library\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Set the system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\":\n",
    "    \"You are a helpful assistant. You reply with very answers.\"\n",
    "}\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = [system_prompt]\n",
    "\n",
    "while True:\n",
    "  # Get user input from the console\n",
    "  user_input = input(\"Ask Q or exit: \")\n",
    "    \n",
    "  if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "  # Append the user input to the chat history\n",
    "  chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  response = client.chat.completions.create(model=\"llama3-70b-8192\",\n",
    "                                            messages=chat_history,\n",
    "                                            max_tokens=100,\n",
    "                                            temperature=1.2)\n",
    "  # Append the response to the chat history\n",
    "  chat_history.append({\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": response.choices[0].message.content\n",
    "  })\n",
    "  # Print the response\n",
    "  print(\"Assistant:\", response.choices[0].message.content) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b752507-910a-4d63-85db-cdd51f003060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'text' to ask a question or 'image' to analyze an image (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq  # Import the Groq library\n",
    "from PIL import Image  # Python Imaging Library for handling images\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Set the system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. You reply with concise answers.\"\n",
    "}\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = [system_prompt]\n",
    "\n",
    "while True:\n",
    "    # Get user input from the console\n",
    "    user_input = input(\"Enter 'text' to ask a question or 'image' to analyze an image (or type 'exit' to quit): \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    if user_input.lower() == 'text':\n",
    "        # Text input scenario\n",
    "        question = input(\"What is your question? \")\n",
    "        # Append the user input to the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "        # Generate a response using the chat completion API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=chat_history,\n",
    "            max_tokens=100,\n",
    "            temperature=1.2\n",
    "        )\n",
    "        \n",
    "        # Append the response to the chat history\n",
    "        chat_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        # Print the response\n",
    "        print(\"Assistant:\", response.choices[0].message.content)\n",
    "\n",
    "    elif user_input.lower() == 'image':\n",
    "        # Image input scenario\n",
    "        image_path = input(\"Enter the path to the image: \")\n",
    "        \n",
    "        try:\n",
    "            # Open and process the image (ensure the image exists)\n",
    "            with Image.open(image_path) as img:\n",
    "                # Here you can perform any image processing if needed\n",
    "                # For example, resizing or converting to a format that the model accepts\n",
    "                # img = img.resize((desired_width, desired_height))\n",
    "\n",
    "                # Assuming Groq has a method for analyzing images\n",
    "                response = client.analyze_image(image=img, model=\"image-analysis-model\")  # Replace with the correct method\n",
    "\n",
    "                # Process the response from the image analysis\n",
    "                analysis_output = response['output']  # Modify based on actual response format\n",
    "\n",
    "                # Print the analysis output\n",
    "                print(\"Image Analysis:\", analysis_output)\n",
    "\n",
    "                # Optionally, append analysis to chat history\n",
    "                chat_history.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": analysis_output\n",
    "                })\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: The image file was not found. Please check the path and try again.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing the image: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid option. Please enter 'text' or 'image'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869bdbc-bba5-496e-8191-de323fee4bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n",
      "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j8s4rgxrfmfscscs9m5bkvqh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 45923. Please try again in 6m39.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceeded. Waiting for 399.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "import time  # Import time module for delay\n",
    "import re  # Import re for regular expression\n",
    "\n",
    "# Function to convert an image to a base64 string\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "# Function to extract wait time from the error message\n",
    "def extract_wait_time(error_message):\n",
    "    # Extract the wait time from the error message using regex\n",
    "    match = re.search(r\"try again in (\\d+)m(\\d+\\.\\d+)s\", error_message)\n",
    "    if match:\n",
    "        minutes = int(match.group(1))\n",
    "        seconds = float(match.group(2))\n",
    "        return minutes * 60 + seconds\n",
    "    return 0\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Path to the image file\n",
    "image_path = \"/Users/mdfarazali/Downloads/_1.jpg\"\n",
    "\n",
    "# Convert the image to a base64 string\n",
    "organ = image_to_base64(image_path)\n",
    "\n",
    "# Create a prompt for image analysis\n",
    "image_prompt = \"Tell me about this picture\"\n",
    "\n",
    "# Main loop to interact with the model\n",
    "while True:\n",
    "    try:\n",
    "        # Create a chat completion request\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": image_prompt},\n",
    "                {\"role\": \"user\", \"content\": organ}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=1.2\n",
    "        )\n",
    "\n",
    "        # Extract and print the model's response\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "        break  # Exit loop after successful request\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"An error occurred: {error_message}\")\n",
    "        \n",
    "        # Check for rate limit error\n",
    "        if \"Rate limit reached\" in error_message:\n",
    "            # Extract wait time from the error message\n",
    "            wait_time = extract_wait_time(error_message)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        else:\n",
    "            break  # Exit on other errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5763cab-e3d0-4803-b6dc-0698f002cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "import time  # Import time module for delay\n",
    "import re  # Import re for regular expression\n",
    "\n",
    "# Function to convert an image to a base64 string\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "# Function to extract wait time from the error message\n",
    "def extract_wait_time(error_message):\n",
    "    # Extract the wait time from the error message using regex\n",
    "    match = re.search(r\"try again in (\\d+)m(\\d+\\.\\d+)s\", error_message)\n",
    "    if match:\n",
    "        minutes = int(match.group(1))\n",
    "        seconds = float(match.group(2))\n",
    "        return minutes * 60 + seconds\n",
    "    return 0\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# Path to the image file\n",
    "image_path = \"/Users/mdfarazali/Downloads/_1.jpg\"\n",
    "\n",
    "# Convert the image to a base64 string\n",
    "organ = image_to_base64(image_path)\n",
    "\n",
    "# Create a prompt for image analysis\n",
    "image_prompt = \"Tell me about this picture\"\n",
    "\n",
    "# Main loop to interact with the model\n",
    "while True:\n",
    "    try:\n",
    "        # Create a chat completion request\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": image_prompt},\n",
    "                {\"role\": \"user\", \"content\": organ}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=1.2\n",
    "        )\n",
    "\n",
    "        # Extract and print the model's response\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "        break  # Exit loop after successful request\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"An error occurred: {error_message}\")\n",
    "        \n",
    "        # Check for rate limit error\n",
    "        if \"Rate limit reached\" in error_message:\n",
    "            # Extract wait time from the error message\n",
    "            wait_time = extract_wait_time(error_message)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time:.2f} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        else:\n",
    "            break  # Exit on other errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd4ac1-242f-44cb-8d4a-5c6f1ec5a5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
